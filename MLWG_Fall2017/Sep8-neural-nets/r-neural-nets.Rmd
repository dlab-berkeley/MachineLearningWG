---
title: "Neural networks in R"
output:
  html_document: default
  html_notebook: default
---

Topics to cover:

* Background
* Single-layer networks
* Multi-layer networks
* Possibly more


Before we dig in, we will install the R packages we'll be using.

**R packages**
```{r}
# List of packages we will use.
packages = c("MASS", "nnet", "h2o", "devtools", "NeuralNetTools")

github_packages = c(
  # Chris's tools package for plotting the SuperLearner.
  "ck37r" = "ck37/ck37r",
  # Use more up-to-date SuperLearner from github.
  "SuperLearner" = "ecpolley/SuperLearner")

devtools::install_github(github_packages)

# Load those github packages.
ck37r::load_packages(names(github_packages))

# Load required non-github packages and install from CRAN if necessary.
ck37r::load_packages(packages, install = T)

# Also install mxnet for potential usage.
# This unfortunately is Mac/Windows only; probably will not work for Linux.
# Actually not working for Mac either.
if (F) {
  # Skip this for now.
  install.packages("drat", repos="https://cran.rstudio.com")
  drat:::addRepo("dmlc")
  install.packages("mxnet")
}

# Could install Keras, but this can get complicated.
if (F) {
  devtools::install_github("rstudio/keras")
  # One version:
  install_keras()
  # Or:
  install_keras(method = "conda")
}

# Clean up variables.
rm(packages, success, github_packages)
```

# Background

Please see Deb's python code for more details on neural network theory.

# Software packages

We'll be using `nnet` for simple neural networks and `h2o` for deep neural networks.

# Data preparation

```{r}
data(Boston, package = "MASS")

# Remove our outcome variable from the covariate list.
X_df = Boston[, -14]

# Convert X from a dataframe to a matrix.
X_mat = model.matrix(~ .,  data = X_df)

# Notice the extra intercept column added by model.matrix.
colnames(X_mat)
                 
#  Remove extra intercept term.
X_mat = X_mat[, -1]

# Regression (continuous) version of our outcome variable.
Y_reg = Boston$medv

# Review outcome distribution.
summary(Y_reg)

# Classification (binary) version of our outcome variable.
Y_class = as.factor(as.numeric(Boston$medv > 23))

# Review outcome distribution.
table(Y_class)
prop.table(table(Y_class))

```

# Single-layer neural network


Quick classification example

```{r}
library(nnet)

# Classification

# Set seed because weights are initialized randomly.
set.seed(1)

# X can be a dataframe or matrix.
# If Y is a factor we need to use this formula notation.
fit = nnet(Y_class ~ X_mat, size = 2, decay = 5e-4, maxit = 200)

# Review our neural network fit.
fit

# Plot our neural network.
library(NeuralNetTools)
plotnet(fit)

# Predict back to our original data.
pred = predict(fit, X_mat)

# Review predictions.
summary(pred)

# 
```

Quick regression example

```{r}
library(nnet)

# Set seed because weights are initialized randomly.
set.seed(1)

# Again, X can be a dataframe or matrix.
fit = nnet(Y_reg ~ X_mat, size = 2, decay = 5e-4, maxit = 200,
           # Enable linear output to support regression.
           linout = T)

# Challenge: try with linout = F (the default) and see what happens.

# Review our neural network fit.
fit

# Visualize neural network.
plotnet(fit)

# Predict back to our original data.
pred = predict(fit, X_mat)

# Review predictions.
summary(pred)

# Calculate mean-squared error (MSE).
mean((pred - Y_reg)^2)

# And root mean squared error (RMSE).
sqrt(mean((pred - Y_reg)^2))

```

# SuperLearner optimization

These challenges can be done in pairs/groups to make it easier.

Challenge 1: use SL.nnet wrapper to estimate performance of the neural network.

Challenge 2: use create.Learner() to test 2, 3, 4, or 5 hidden units and create a weighted average ensemble.

# Multi-layer neural network

Challenge: use h2o to design this.

```{r}
library(h2o)
# Startup and connect to our existing h2o cluster.
# Use all available threads.
h2o.init(nthreads = -1)

# Clean slate - just in case the cluster was already running.
h2o.removeAll()

# Load x data into h2o.
data = as.h2o(cbind(X_df, `_outcome` = Y_reg))
dim(data)

outcome = "_outcome"
x = colnames(X_df)

# Fit the deep learning model here.

# Shutdown server when we're done.
h2o.shutdown(prompt = F)
```

See also Erin LeDell's [excellent tutorial on deep learning](https://github.com/ledell/useR-machine-learning-tutorial/blob/master/deep-neural-networks.Rmd).
